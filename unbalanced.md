# 解决样本不平衡问题

1. SMOTE(Synthetic Minority Over-sampling Technique)过采样小样本（扩充小类，产生新数据）
  即该算法构造的数据是新样本，原数据集中不存在的。该基于距离度量选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了更多的新生数据。（优点是相当于合理地对小样本的分类平面进行的一定程度的外扩；也相当于对小类错分进行加权惩罚（解释见3））


2. 欠采样大样本（压缩大类，产生新数据）
设小类中有N个样本。将大类聚类成N个簇，然后使用每个簇的中心组成大类中的N个样本，加上小类中所有的样本进行训练。（优点是保留了大类在特征空间的分布特性，又降低了大类数据的数目）


3. 对小类错分进行加权惩罚
对分类器的小类样本数据增加权值，降低大类样本的权值（这种方法其实是产生了新的数据分布，即产生了新的数据集，译者注），从而使得分类器将重点集中在小类样本身上。一个具体做法就是，在训练分类器时，若分类器将小类样本分错时额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加“关心”小类样本。如penalized-SVM和penalized-LDA算法。
对小样本进行过采样（例如含L倍的重复数据），其实在计算小样本错分cost functions时会累加L倍的惩罚分数。


4. 分治ensemble
将大类中样本聚类到L个聚类中，然后训练L个分类器；每个分类器使用大类中的一个簇与所有的小类样本进行训练得到；最后对这L个分类器采取少数服从多数对未知类别数据进行分类，如果是连续值（预测），那么采用平均值。


5. 分层级ensemble
使用原始数据集训练第一个学习器L1；将L1错分的数据集作为新的数据集训练L2；将L1和L2分类结果不一致的数据作为数据集训练L3；最后测试集上将三个分类器的结果汇总（结合这三个分类器，采用投票的方式来决定分类结果，因此只有当L2与L3都分类为false时，最终结果才为false，否则true。）


6. 基于异常检测的分类
   用异常检测算法（如高斯混合模型、聚类等）检测得到离群点或异常点；再对这些异常点为训练集学习一个分类器。

